---
name: Tyk Demo API Tests (Parallel)
on:
  workflow_dispatch:
    inputs:
      gateway_debug:
        description: 'Enable Gateway debug logging'
        required: false
        type: boolean
        default: false
      dashboard_debug:
        description: 'Enable Dashboard debug logging'
        required: false
        type: boolean
        default: false
  push:

jobs:
  # First job to collect all the deployments to test
  discover-deployments:
    runs-on: ubuntu-latest
    outputs:
      deployment-matrix: ${{ steps.set-matrix.outputs.deployment-matrix }}
      gateway-tag: ${{ steps.get-gateway-tag.outputs.gateway-tag }}
    steps:
      - name: Check Out Repository Code
        uses: actions/checkout@v4

      - name: Find All Deployments
        id: set-matrix
        run: |
          DEPLOYMENTS=$(find deployments -mindepth 1 -maxdepth 1 -type d -printf '%f\n' | jq -R -s -c 'split("\n") | map(select(length > 0)) | sort')

          if [ "${#DEPLOYMENTS[@]}" -eq 0 ]; then
            echo "::error::No deployments found. Exiting workflow." >&2
            exit 1
          fi

          echo "deployment-matrix=${DEPLOYMENTS}" >> $GITHUB_OUTPUT
          echo "âœ… Found deployments: ${DEPLOYMENTS}"
      - name: Extract Docker Image Tag
        id: get-gateway-tag
        run: |
          GATEWAY_TAG=$(awk '/tyk-gateway:/ { in_gateway=1 }
                            in_gateway && /image:/ {
                              if (match($0, /:-v[0-9]+\.[0-9]+\.[0-9]+([-._a-zA-Z0-9]*)?/)) {
                                print substr($0, RSTART+2, RLENGTH-2)
                              }
                              in_gateway=0
                            }' deployments/tyk/docker-compose.yml | sort -u | head -n 1)

          if [[ -z "$GATEWAY_TAG" ]]; then
            echo "::error::GATEWAY_TAG is empty"
            exit 1
          fi

          if [[ ! "$GATEWAY_TAG" =~ ^v[0-9]+\.[0-9]+\.[0-9]+([-._a-zA-Z0-9]+)?$ ]]; then
            echo "::error::GATEWAY_TAG '$GATEWAY_TAG' is not in a valid semver format"
            exit 1
          fi

          echo "âœ… Extracted gateway tag: $GATEWAY_TAG"
          echo "gateway-tag=$GATEWAY_TAG" >> "$GITHUB_OUTPUT"

  # Run each deployment test in parallel
  test-deployments:
    needs: discover-deployments
    runs-on: ubuntu-latest
    outputs:
      deployment-results: ${{ steps.set-results.outputs.results }}
    strategy:
      fail-fast: false
      matrix:
        deployment: ${{ fromJson(needs.discover-deployments.outputs.deployment-matrix) }}

    steps:
      - name: Check Out Repository Code
        uses: actions/checkout@v4

      - name: Set global env vars
        run: |
          echo "DEPLOYMENT_DIR=$(pwd)/deployments/${{ matrix.deployment }}" >> $GITHUB_ENV
          echo "BASE_DIR=$(pwd)" >> $GITHUB_ENV

      - name: Install websocat for tests
        run: |
          wget https://github.com/vi/websocat/releases/download/v1.14.0/websocat.x86_64-unknown-linux-musl
          chmod +x websocat.x86_64-unknown-linux-musl
          sudo mv websocat.x86_64-unknown-linux-musl /usr/local/bin/websocat

      - name: Set up environment
        run: |
          echo "::group::Preparing test logs"
          source "$(pwd)/scripts/test-common.sh"
          prepare_test_logs
          echo "::endgroup::"

          echo "::group::Defaulting to debug logs for Gateway and Dashboard"
          echo "GATEWAY_LOGLEVEL=debug" >> .env
          echo "DASHBOARD_LOGLEVEL=debug" >> .env
          echo "âœ… Gateway and Dashboard log levels set to debug ğŸ›"
          echo "::endgroup::"

          echo "::group::Setting licences"
          if [ -z "${{ secrets.DASH_LICENSE }}" ]; then
            echo "::error::DASH_LICENSE secret is not set or is empty." >&2
            exit 1
          fi
          if [ -z "${{ secrets.MDCB_LICENSE }}" ]; then
            echo "::error::MDCB_LICENSE secret is not set or is empty." >&2
            exit 1
          fi
          echo "âœ… DASHBOARD_LICENCE added to .env"
          echo "DASHBOARD_LICENCE=${{ secrets.DASH_LICENSE }}" >> .env
          echo "âœ… MDCB_LICENCE added to .env"
          echo "MDCB_LICENCE=${{ secrets.MDCB_LICENSE }}" >> .env
          echo "::endgroup::"

      - name: Read Deployment Manifest
        id: read-manifest
        run: |
          skip_deployment="false"
          compose_argument=""

          if [ -f "${DEPLOYMENT_DIR}/deployment.json" ]; then
            echo "Deployment manifest found in ${DEPLOYMENT_DIR}"
            jq '.' "${DEPLOYMENT_DIR}/deployment.json"
            skip_deployment=$(jq -r '.github.skipDeployment // "false"' "${DEPLOYMENT_DIR}/deployment.json")
            compose_argument=$(jq -r '.composeArgument // empty' "${DEPLOYMENT_DIR}/deployment.json")
          else
            echo "Deployment manifest not found in ${DEPLOYMENT_DIR}"
          fi

          echo "skip_deployment=$skip_deployment" >> $GITHUB_OUTPUT
          echo "compose_argument=$compose_argument" >> $GITHUB_OUTPUT

      - name: Cache Go Plugins
        uses: actions/cache@v4
        with:
          path: .bootstrap/plugin-cache/${{ needs.discover-deployments.outputs.gateway-tag }}
          key: ${{ runner.os }}-plugin-cache-${{ needs.discover-deployments.outputs.gateway-tag }}-${{ hashFiles('deployments/tyk/volumes/tyk-gateway/plugins/go/**/*') }}

      - name: Create deployment
        id: create-deployment
        if: steps.read-manifest.outputs.skip_deployment != 'true'
        run: |
          COMPOSE_ARG="${{ steps.read-manifest.outputs.compose_argument }}"

          if ./up.sh "${COMPOSE_ARG:-${{ matrix.deployment }}}" --hide-progress --skip-hostname-check; then
            echo "âœ… Deployment created successfully"
          else
            echo "::error::Deployment creation failed for ${{ matrix.deployment }}"
            exit 1
          fi

      - name: Validate Tests
        if: steps.read-manifest.outputs.skip_deployment != 'true'
        id: validate-tests
        run: |
          source "$(pwd)/scripts/test-common.sh"

          echo "::group::Checking for Postman tests"
          if validate_postman_collection "${{ matrix.deployment }}" "${DEPLOYMENT_DIR}"; then
            echo "âœ… Found Postman tests"
            echo "postman-tests-found=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ No Postman tests found"
            echo "postman-tests-found=false" >> $GITHUB_OUTPUT
          fi
          echo "::endgroup::"

          echo "::group::Checking for custom tests"
          if validate_test_scripts "${{ matrix.deployment }}" "${DEPLOYMENT_DIR}"; then
            echo "âœ… Found custom tests"
            echo "custom-tests-found=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ No custom tests found"
            echo "custom-tests-found=false" >> $GITHUB_OUTPUT
          fi
          echo "::endgroup::"

      - name: Run Postman Tests
        id: run-postman-tests
        if: steps.validate-tests.outputs.postman-tests-found == 'true'
        continue-on-error: true
        run: |
          source "$(pwd)/scripts/test-common.sh"

          if run_postman_test "${{ matrix.deployment }}" "${DEPLOYMENT_DIR}"; then
            echo "âœ… Postman tests passed"
          else
            echo "âŒ Postman tests failed"
          fi

          strip_control_chars "logs/postman.log"

      - name: Run Custom Tests
        id: run-custom-tests
        if: steps.validate-tests.outputs.custom-tests-found == 'true'
        continue-on-error: true
        run: |
          source "$(pwd)/scripts/test-common.sh"

          # Run custom tests
          if run_test_scripts "${{ matrix.deployment }}" "${DEPLOYMENT_DIR}"; then
            echo "âœ… Custom tests passed"
          else
            echo "âŒ Custom tests failed"
          fi

      - name: Capture Container Logs
        if: always()
        run: |
          source "$(pwd)/scripts/test-common.sh"

          capture_container_logs ${{ matrix.deployment }}

      - name: Upload Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ matrix.deployment }}
          path: logs/

      - name: Set Results
        id: set-results
        if: always()
        run: |
          # Determine deployment status - success, skipped, or failure
          DEPLOYMENT_STATUS="failure"
          if [ "${{ steps.read-manifest.outputs.skip_deployment }}" == "true" ]; then
            DEPLOYMENT_STATUS="skipped"
          elif [ "${{ steps.create-deployment.outcome }}" == "success" ]; then
            DEPLOYMENT_STATUS="success"
          fi

          # Capture Postman test info
          POSTMAN_FOUND=${{ steps.validate-tests.outputs.postman-tests-found || 'false' }}
          POSTMAN_STATUS="skipped"
          if [ "$POSTMAN_FOUND" == "true" ]; then
            POSTMAN_STATUS=${{ steps.run-postman-tests.outcome }}
          fi

          # Capture custom test info
          CUSTOM_FOUND=${{ steps.validate-tests.outputs.custom-tests-found || 'false' }}
          CUSTOM_STATUS="skipped"
          if [ "$CUSTOM_FOUND" == "true" ]; then
            CUSTOM_STATUS=${{ steps.run-custom-tests.outcome }}
          fi

          # Create JSON result for this deployment
          RESULT=$(echo '{
            "deployment": "${{ matrix.deployment }}",
            "deployment_status": "'$DEPLOYMENT_STATUS'",
            "postman_found": "'$POSTMAN_FOUND'",
            "postman_status": "'$POSTMAN_STATUS'",
            "custom_found": "'$CUSTOM_FOUND'",
            "custom_status": "'$CUSTOM_STATUS'"
          }' | jq -c .)

          echo "$RESULT" | jq
          echo "results=$RESULT" >> $GITHUB_OUTPUT

  # Collect test results and summarize
  summarise-results:
    needs: [discover-deployments, test-deployments]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Summarise test results
        run: |
          # Create a markdown version for artifact
          echo "## Test Results Summary" > summary.md
          echo "" >> summary.md
          echo "| Deployment | Deployment Status | Postman Tests | Custom Tests | Overall Status |" >> summary.md
          echo "|------------|-------------------|---------------|--------------|----------------|" >> summary.md

          # Create a monospaced version for GitHub Actions console
          echo "::group::Test Results Summary"
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘ DEPLOYMENT     â•‘ DEPLOYMENT STATUS â•‘ POSTMAN TESTS  â•‘ CUSTOM TESTS  â•‘ OVERALL STATUS   â•‘"
          echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"

          # Process deployment results
          DEPLOYMENTS='${{ needs.discover-deployments.outputs.deployment-matrix }}'
          RESULTS_JSON=$(echo '${{ toJSON(needs.test-deployments.outputs) }}' | jq -r '.["deployment-results"]')

          # For each deployment, find its result
          for deployment in $(echo "$DEPLOYMENTS" | jq -r '.[]'); do
            # Find the corresponding result or use default values if not found
            DEPLOYMENT_RESULT="{}"

            # Loop through results to find the matching deployment
            while read -r result; do
              if [[ $(echo "$result" | jq -r '.deployment') == "$deployment" ]]; then
                DEPLOYMENT_RESULT="$result"
                break
              fi
            done < <(echo "$RESULTS_JSON" | jq -c '.')

            # Get the statuses (with defaults)
            DEPLOYMENT_STATUS=$(echo "$DEPLOYMENT_RESULT" | jq -r '.deployment_status // "failure"')
            POSTMAN_FOUND=$(echo "$DEPLOYMENT_RESULT" | jq -r '.postman_found // "false"')
            POSTMAN_STATUS=$(echo "$DEPLOYMENT_RESULT" | jq -r '.postman_status // "failure"')
            CUSTOM_FOUND=$(echo "$DEPLOYMENT_RESULT" | jq -r '.custom_found // "false"')
            CUSTOM_STATUS=$(echo "$DEPLOYMENT_RESULT" | jq -r '.custom_status // "failure"')

            # Format deployment status (now handles skipped deployments)
            if [[ "$DEPLOYMENT_STATUS" == "skipped" ]]; then
              DEPLOYMENT_DISPLAY="â– Skipped"
              DEPLOYMENT_CONSOLE="- Skipped"
            elif [[ "$DEPLOYMENT_STATUS" == "success" ]]; then
              DEPLOYMENT_DISPLAY="âœ… Success"
              DEPLOYMENT_CONSOLE="âœ“ Success"
            else
              DEPLOYMENT_DISPLAY="âŒ Failed"
              DEPLOYMENT_CONSOLE="âœ— Failed"
            fi

            # Format Postman status
            if [[ "$DEPLOYMENT_STATUS" == "skipped" ]]; then
              POSTMAN_DISPLAY="â– Skipped"
              POSTMAN_CONSOLE="- Skipped"
            elif [[ "$POSTMAN_STATUS" == "success" ]]; then
              POSTMAN_DISPLAY="âœ… Passed"
              POSTMAN_CONSOLE="âœ“ Passed"
            else
              POSTMAN_DISPLAY="âŒ Failed"
              POSTMAN_CONSOLE="âœ— Failed"
            fi

            # Format Custom tests status
            if [[ "$DEPLOYMENT_STATUS" == "skipped" ]]; then
              CUSTOM_DISPLAY="â– Skipped"
              CUSTOM_CONSOLE="- Skipped"
            elif [[ "$CUSTOM_STATUS" == "success" ]]; then
              CUSTOM_DISPLAY="âœ… Passed"
              CUSTOM_CONSOLE="âœ“ Passed"
            else
              CUSTOM_DISPLAY="âŒ Failed"
              CUSTOM_CONSOLE="âœ— Failed"
            fi

            # Determine overall status
            if [[ "$DEPLOYMENT_STATUS" == "skipped" ]]; then
              OVERALL_DISPLAY="â– Skipped"
              OVERALL_CONSOLE="- Skipped"
            elif [[ "$DEPLOYMENT_STATUS" == "failure" ]]; then
              OVERALL_DISPLAY="âŒ Failed (deployment)"
              OVERALL_CONSOLE="âœ— Failed (deployment)"
            elif [[ "$POSTMAN_FOUND" == "true" && "$POSTMAN_STATUS" != "success" ]] || [[ "$CUSTOM_FOUND" == "true" && "$CUSTOM_STATUS" != "success" ]]; then
              OVERALL_DISPLAY="âŒ Failed (tests)"
              OVERALL_CONSOLE="âœ— Failed (tests)"
            else
              OVERALL_DISPLAY="âœ… Passed"
              OVERALL_CONSOLE="âœ“ Passed"
            fi

            # Add to markdown file
            echo "| $deployment | $DEPLOYMENT_DISPLAY | $POSTMAN_DISPLAY | $CUSTOM_DISPLAY | $OVERALL_DISPLAY |" >> summary.md

            # Add to console output (pad strings for consistent column width)
            DEPLOY_PAD=$(printf "%-14s" "$deployment")
            DEPLOYMENT_PAD=$(printf "%-17s" "$DEPLOYMENT_CONSOLE")
            POSTMAN_PAD=$(printf "%-14s" "$POSTMAN_CONSOLE")
            CUSTOM_PAD=$(printf "%-13s" "$CUSTOM_CONSOLE")
            OVERALL_PAD=$(printf "%-16s" "$OVERALL_CONSOLE")

            echo "â•‘ $DEPLOY_PAD â•‘ $DEPLOYMENT_PAD â•‘ $POSTMAN_PAD â•‘ $CUSTOM_PAD â•‘ $OVERALL_PAD â•‘"
          done

          # Close the console table
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "::endgroup::"

          # Print the markdown for visibility
          echo "Markdown summary generated and saved to artifact."

      - name: Upload Summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: summary.md
